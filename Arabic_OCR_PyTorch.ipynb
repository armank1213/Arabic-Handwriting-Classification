{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicCharDataset(Dataset):\n",
    "    def __init__(self, image_file, label_file, transform=None):\n",
    "        self.images = pd.read_csv(image_file, header=None).values\n",
    "        self.labels = pd.read_csv(label_file, header=None).values.ravel() - 1\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].reshape(32, 32).astype(np.float32) / 255.0\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "class OptimizedArabicCharNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(OptimizedArabicCharNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128 * 4 * 4, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and Setup\n",
    "num_classes = 28 \n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 20  \n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomRotation(5),  \n",
    "    transforms.RandomAffine(0, shear=5, scale=(0.9, 1.1)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "train_dataset = ArabicCharDataset('data/csvTrainImages 13440x1024.csv', 'data/csvTrainLabel 13440x1.csv', transform=transform)\n",
    "test_dataset = ArabicCharDataset('data/csvTestImages 3360x1024.csv', 'data/csvTestLabel 3360x1.csv', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Epoch [1/20], Train Loss: 1.9599, Val Loss: 0.8350, Accuracy: 73.37%\n",
      "Epoch [2/20], Train Loss: 0.8391, Val Loss: 0.4944, Accuracy: 82.72%\n",
      "Epoch [3/20], Train Loss: 0.6216, Val Loss: 0.3689, Accuracy: 87.48%\n",
      "Epoch [4/20], Train Loss: 0.5041, Val Loss: 0.5546, Accuracy: 83.06%\n",
      "Epoch [5/20], Train Loss: 0.4514, Val Loss: 0.2936, Accuracy: 89.93%\n",
      "Epoch [6/20], Train Loss: 0.4172, Val Loss: 0.3375, Accuracy: 88.93%\n",
      "Epoch [7/20], Train Loss: 0.3588, Val Loss: 0.9740, Accuracy: 72.52%\n",
      "Epoch [8/20], Train Loss: 0.3306, Val Loss: 0.2601, Accuracy: 91.72%\n",
      "Epoch [9/20], Train Loss: 0.3096, Val Loss: 0.3092, Accuracy: 90.56%\n",
      "Epoch [10/20], Train Loss: 0.3047, Val Loss: 0.3544, Accuracy: 87.57%\n",
      "Epoch [11/20], Train Loss: 0.2825, Val Loss: 0.4858, Accuracy: 85.85%\n",
      "Epoch [12/20], Train Loss: 0.2252, Val Loss: 0.1803, Accuracy: 94.67%\n",
      "Epoch [13/20], Train Loss: 0.2155, Val Loss: 0.1673, Accuracy: 94.89%\n",
      "Epoch [14/20], Train Loss: 0.1840, Val Loss: 0.1660, Accuracy: 94.84%\n",
      "Epoch [15/20], Train Loss: 0.1869, Val Loss: 0.1698, Accuracy: 95.00%\n",
      "Epoch [16/20], Train Loss: 0.1846, Val Loss: 0.1636, Accuracy: 95.20%\n",
      "Epoch [17/20], Train Loss: 0.1662, Val Loss: 0.1703, Accuracy: 94.98%\n",
      "Epoch [18/20], Train Loss: 0.1646, Val Loss: 0.1458, Accuracy: 95.40%\n",
      "Epoch [19/20], Train Loss: 0.1585, Val Loss: 0.1426, Accuracy: 95.58%\n",
      "Epoch [20/20], Train Loss: 0.1622, Val Loss: 0.1519, Accuracy: 95.20%\n",
      "Test Accuracy: 94.73%\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Epoch [1/20], Train Loss: 1.8620, Val Loss: 0.7443, Accuracy: 76.23%\n",
      "Epoch [2/20], Train Loss: 0.8541, Val Loss: 0.4817, Accuracy: 84.64%\n",
      "Epoch [3/20], Train Loss: 0.6269, Val Loss: 0.3865, Accuracy: 87.03%\n",
      "Epoch [4/20], Train Loss: 0.5395, Val Loss: 0.4100, Accuracy: 84.35%\n",
      "Epoch [5/20], Train Loss: 0.4504, Val Loss: 0.7155, Accuracy: 73.42%\n",
      "Epoch [6/20], Train Loss: 0.3955, Val Loss: 0.2773, Accuracy: 91.12%\n",
      "Epoch [7/20], Train Loss: 0.3547, Val Loss: 0.4319, Accuracy: 86.88%\n",
      "Epoch [8/20], Train Loss: 0.3341, Val Loss: 0.2868, Accuracy: 90.51%\n",
      "Epoch [9/20], Train Loss: 0.3064, Val Loss: 0.3387, Accuracy: 90.27%\n",
      "Epoch [10/20], Train Loss: 0.2479, Val Loss: 0.1854, Accuracy: 94.35%\n",
      "Epoch [11/20], Train Loss: 0.2350, Val Loss: 0.1717, Accuracy: 94.08%\n",
      "Epoch [12/20], Train Loss: 0.1997, Val Loss: 0.1788, Accuracy: 94.44%\n",
      "Epoch [13/20], Train Loss: 0.2026, Val Loss: 0.1662, Accuracy: 95.00%\n",
      "Epoch [14/20], Train Loss: 0.1890, Val Loss: 0.1696, Accuracy: 94.49%\n",
      "Epoch [15/20], Train Loss: 0.1969, Val Loss: 0.1670, Accuracy: 95.20%\n",
      "Epoch [16/20], Train Loss: 0.1747, Val Loss: 0.1631, Accuracy: 94.62%\n",
      "Epoch [17/20], Train Loss: 0.1734, Val Loss: 0.1567, Accuracy: 95.02%\n",
      "Epoch [18/20], Train Loss: 0.1734, Val Loss: 0.1543, Accuracy: 95.25%\n",
      "Epoch [19/20], Train Loss: 0.1634, Val Loss: 0.1595, Accuracy: 95.04%\n",
      "Epoch [20/20], Train Loss: 0.1734, Val Loss: 0.1494, Accuracy: 95.58%\n",
      "Test Accuracy: 95.65%\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Epoch [1/20], Train Loss: 1.9190, Val Loss: 0.7667, Accuracy: 74.38%\n",
      "Epoch [2/20], Train Loss: 0.8302, Val Loss: 0.4687, Accuracy: 84.35%\n",
      "Epoch [3/20], Train Loss: 0.6028, Val Loss: 0.4139, Accuracy: 86.27%\n",
      "Epoch [4/20], Train Loss: 0.4908, Val Loss: 0.3121, Accuracy: 89.80%\n",
      "Epoch [5/20], Train Loss: 0.4378, Val Loss: 0.3103, Accuracy: 89.84%\n",
      "Epoch [6/20], Train Loss: 0.4059, Val Loss: 0.5638, Accuracy: 78.33%\n",
      "Epoch [7/20], Train Loss: 0.3737, Val Loss: 0.2282, Accuracy: 92.68%\n",
      "Epoch [8/20], Train Loss: 0.3608, Val Loss: 0.2870, Accuracy: 89.84%\n",
      "Epoch [9/20], Train Loss: 0.3272, Val Loss: 0.3680, Accuracy: 88.30%\n",
      "Epoch [10/20], Train Loss: 0.2965, Val Loss: 0.2205, Accuracy: 93.06%\n",
      "Epoch [11/20], Train Loss: 0.2824, Val Loss: 0.2250, Accuracy: 92.92%\n",
      "Epoch [12/20], Train Loss: 0.2705, Val Loss: 0.2904, Accuracy: 91.65%\n",
      "Epoch [13/20], Train Loss: 0.2595, Val Loss: 0.3567, Accuracy: 90.02%\n",
      "Epoch [14/20], Train Loss: 0.2186, Val Loss: 0.1612, Accuracy: 95.04%\n",
      "Epoch [15/20], Train Loss: 0.1918, Val Loss: 0.1549, Accuracy: 95.38%\n",
      "Epoch [16/20], Train Loss: 0.1786, Val Loss: 0.1387, Accuracy: 95.62%\n",
      "Epoch [17/20], Train Loss: 0.1824, Val Loss: 0.1507, Accuracy: 95.16%\n",
      "Epoch [18/20], Train Loss: 0.1682, Val Loss: 0.1442, Accuracy: 95.22%\n",
      "Epoch [19/20], Train Loss: 0.1560, Val Loss: 0.1494, Accuracy: 95.36%\n",
      "Epoch [20/20], Train Loss: 0.1700, Val Loss: 0.1479, Accuracy: 95.31%\n",
      "Test Accuracy: 95.48%\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation setup\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs, patience=5):\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "            break\n",
    "\n",
    "    return model, best_accuracy\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Cross-validation training\n",
    "best_fold_accuracy = 0.0\n",
    "best_fold_model = None\n",
    "\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    train_subsampler = SubsetRandomSampler(train_ids)\n",
    "    val_subsampler = SubsetRandomSampler(val_ids)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_subsampler)\n",
    "    val_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=val_subsampler)\n",
    "    \n",
    "    model = OptimizedArabicCharNet(num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n",
    "\n",
    "    # Train the model\n",
    "    model, fold_accuracy = train_with_early_stopping(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs)\n",
    "\n",
    "    # Update best model if this fold's accuracy is higher\n",
    "    if fold_accuracy > best_fold_accuracy:\n",
    "        best_fold_accuracy = fold_accuracy\n",
    "        best_fold_model = model.state_dict()\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    evaluate(model, test_loader, device)\n",
    "\n",
    "# Save the best model from all folds\n",
    "torch.save(best_fold_model, 'Arabic_OCR_PyTorch.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation on Test Set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3s/pl6fx3pd1xg5qc2ntf5ztdfw0000gn/T/ipykernel_7861/3298790117.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  final_model.load_state_dict(torch.load('Arabic_OCR_PyTorch.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 95.12%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation on the test set using the best model from the last fold\n",
    "print(\"Final Evaluation on Test Set:\")\n",
    "final_model = OptimizedArabicCharNet(num_classes).to(device)\n",
    "final_model.load_state_dict(torch.load('Arabic_OCR_PyTorch.pth'))\n",
    "evaluate(final_model, test_loader, device)\n",
    "\n",
    "def classify_image(model, csv_file, row_index, transform, device):\n",
    "    model.eval()\n",
    "    df = pd.read_csv(csv_file, header=None)\n",
    "    image = df.iloc[row_index].values.reshape(32, 32).astype(np.float32) / 255.0\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    \n",
    "    return f\"Detected Arabic character: {predicted.item() + 1}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character mapping\n",
    "arabic_chars = 'أبتثجحخدذرزسشصضطظعغفقكلمنهوي'\n",
    "arabic_characters = ['alef', 'beh', 'teh', 'theh', 'jeem', 'hah', 'khah', 'dal', 'thal',\n",
    "                    'reh', 'zain', 'seen', 'sheen', 'sad', 'dad', 'tah', 'zah', 'ain',\n",
    "                    'ghain', 'feh', 'qaf', 'kaf', 'lam', 'meem', 'noon', 'heh', 'waw', 'yeh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Arabic character: 8\n",
      "The detected Arabic character is: د\n",
      "The detected Arabic character (in English) is: dal\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "result = classify_image(model, 'data/csvTestImages 3360x1024.csv', 16, transform, device)\n",
    "print(result)\n",
    "\n",
    "# Map the result to Arabic character and English transliteration\n",
    "result_index = int(result.split()[-1]) - 1\n",
    "if 0 <= result_index < len(arabic_chars):\n",
    "    print(f\"The detected Arabic character is: {arabic_chars[result_index]}\")\n",
    "    print(f\"The detected Arabic character (in English) is: {arabic_characters[result_index]}\")\n",
    "else:\n",
    "    print(f\"Error: Invalid index {result_index}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
